{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singapore Public Housing (HDB) Resale Price Prediction Model (Part 6)\n",
    "### Feature Engineering - Hexagon Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies for Hexagon-Map Creation\n",
    "# # No need to import\n",
    "# !apt install libspatialindex-dev\n",
    "# !pip install rtree, geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "from matplotlib.patches import RegularPolygon\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = pd.read_csv('./Dataset/Transitional/final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hexagon Map Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate haversine distance between two coordinates\n",
    "def haversine(coord1, coord2):\n",
    "    lon1, lat1 = coord1\n",
    "    lon2, lat2 = coord2\n",
    "    \n",
    "    # Earth radius\n",
    "    R = 6_371_000\n",
    "    phi_1 = np.radians(lat1)\n",
    "    phi_2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a),np.sqrt(1 - a))\n",
    "    meters = R * c\n",
    "    km = meters / 1000.0\n",
    "    meters = round(meters)\n",
    "    km = round(km, 3)\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# # Loading data using Geopandas\n",
    "# df = gpd.read_file('./Raw/planning_area_with_whampoa.json')\n",
    "\n",
    "# # Drop off planning area that are not HDB estate to save up processing power and memory\n",
    "# df = df.drop([0, 1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 16, 18, 19, 25, 26, 29, 30, 34, 39, 44, 51, 53]).reset_index(drop=True)\n",
    "\n",
    "# # Combining smaller planning areas in accordance of HDB estate neighbourhood in raw DataFrame\n",
    "# df.loc[[0, 25, 26, 29, 30], 'name'] = 'CENTRAL AREA'\n",
    "# df.loc[[1, 31], 'name'] = 'KALLANG/WHAMPOA'\n",
    "\n",
    "# # Dissolve area with the same name\n",
    "# TO_boundary = df.dissolve(by='name')\n",
    "\n",
    "# # Export to geojson file for further processing\n",
    "# TO_boundary.to_file('./Dataset/Spatial/planning_area_optimized.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hex_map(hex_diam=250):\n",
    "    # Import the optimized planning area polygons\n",
    "    df = gpd.read_file('./Dataset/Spatial/planning_area_optimized.geojson')\n",
    "\n",
    "    # Furthest coordinates on Singapore\n",
    "    xmin, ymin, xmax ,ymax = df.total_bounds\n",
    "\n",
    "    # East-West & North-South Length of Singapore\n",
    "    EW = haversine((xmin, ymin), (xmax, ymin))\n",
    "    NS = haversine((xmin, ymin), (xmin, ymax)) + 10000\n",
    "\n",
    "    # Diameter of hexagon in meters\n",
    "    d = hex_diam\n",
    "\n",
    "    # Calculate width of hexagon in meters and grid structure of hexagon\n",
    "    w = d * np.sin(np.pi / 3)\n",
    "    n_cols = int(EW / w) + 1\n",
    "    n_rows = int(NS / d) + 1\n",
    "\n",
    "    # Calculate width of hexagon in coordinates\n",
    "    w = (xmax - xmin) / n_cols\n",
    "\n",
    "    # Calculate diameter of hexagon in coordinates\n",
    "    d = w / np.sin(np.pi / 3)\n",
    "\n",
    "    array_of_hexes = []\n",
    "\n",
    "    for rows in range(0, n_rows):\n",
    "        hcoord = np.arange(xmin, xmax, w) + (rows % 2) * w / 2\n",
    "        vcoord = [ymax - rows * d * 0.75] * n_cols\n",
    "        for x, y in zip(hcoord, vcoord):\n",
    "            hexes = RegularPolygon((x, y), numVertices=6, radius=d/2, alpha=0.2, edgecolor='k')\n",
    "            verts = hexes.get_path().vertices\n",
    "            trans = hexes.get_patch_transform()\n",
    "            points = trans.transform(verts)\n",
    "            array_of_hexes.append(Polygon(points))\n",
    "\n",
    "    hex_grid = gpd.GeoDataFrame({'geometry': array_of_hexes}, crs={'init': 'epsg:4326'})\n",
    "    TO_hex = gpd.overlay(hex_grid, df)\n",
    "    TO_hex = gpd.GeoDataFrame(TO_hex,geometry='geometry')\n",
    "    TO_hex.to_file('./Dataset/Spatial/planning_area_hexed.geojson', driver='GeoJSON')\n",
    "    TO_hex[['geometry']].to_csv('./Dataset/Spatial/HexMap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# create_hex_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in hex-ed singapore map into Geopandas DataFrame\n",
    "hex_map = pd.read_csv('./Dataset/Spatial/HexMap.csv')\n",
    "hex_map['geometry'] = hex_map['geometry'].apply(wkt.loads)\n",
    "hex_map = gpd.GeoDataFrame(hex_map, geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove terrace-house (outliers) from dataset for better mapping visualization\n",
    "hdb = hdb[hdb['flat_model']!='Terrace'].reset_index(drop=True)\n",
    "\n",
    "# Price per square feet\n",
    "hdb['price_per_sqft'] = hdb['resale_price'] / hdb['floor_area_sqm'] * 10.7639\n",
    "\n",
    "# Unit Price --- Price per area per year lease\n",
    "hdb['unit_price'] = hdb['price_per_sqft'] / hdb['remaining_lease']\n",
    "\n",
    "# Elementary Price --- Price per area per year lease per storey level\n",
    "hdb['elem_price'] = hdb['unit_price'] / hdb['storey_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DATA IS COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# # Original method of grouping property entries by address\n",
    "# hex_df = hdb.groupby('address').mean().reset_index()\n",
    "\n",
    "# Renewed method to bypass calculation running\n",
    "hex_df = pd.read_csv('./Dataset/final_data_with_hex.csv')\n",
    "\n",
    "# Label each address to their respective hexagon\n",
    "if hex_df['hex_id'].isnull().sum() > 0:\n",
    "    for idx, (latitude, longitude) in enumerate(zip(hex_df['latitude'], hex_df['longitude'])):\n",
    "        print('\\rWaiting... {} addresses remaining... '.format(len(hex_df)-idx-1), end='.')\n",
    "        if math.isnan(hex_df.loc[idx, 'hex_id']):\n",
    "            for boundary, hex_id in zip(hex_map.geometry, hex_map.hex_id):\n",
    "                coor = Point(longitude, latitude)\n",
    "                if coor.within(boundary):\n",
    "                    hex_df.loc[idx, 'hex_id'] = hex_id\n",
    "                    break\n",
    "            hex_df.to_csv('./Dataset/final_data_with_hex.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print('--- DATA IS COMPLETE ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Optimize by taking only hexagon with address/data\n",
    "hex_map.columns = ['hex_id', 'geometry']\n",
    "hex_map = hex_map[hex_map['hex_id'].isin(hex_df['hex_id'].unique())]\n",
    "\n",
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# # Export for Colab\n",
    "# hex_map.to_csv('./Dataset/Spatial/HexMap_Optimized.csv', index=False)\n",
    "\n",
    "# # Read from Colab\n",
    "# hex_map = pd.read_csv('./Dataset/Spatial/HexMap_Optimized.csv')\n",
    "# hex_map['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# hex_map = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# # Export the optimized hexmap to geojson format for mapping purpose\n",
    "# hex_map.columns = ['hex_id', 'geometry']\n",
    "# hex_map.to_file('./Dataset/Spatial/hexmap.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any missing points not hex-ed (outliers far from actual HDB estate)\n",
    "# Only 3 addresses affect - 1 x Changi Village + 2 x Farrer Road\n",
    "hex_df = hex_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Only merged if column not exist - to ease re-running of code\n",
    "if 'town' not in hex_df.columns:\n",
    "    hex_df = pd.merge(hex_df, hdb[['address', 'town']], how='left', on='address')\n",
    "    \n",
    "hex_df = hex_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate HDB-town-wide local Minimum and Maximum\n",
    "town_ave = hex_df.groupby('town').agg(['min', 'max'])['unit_price'].reset_index()\n",
    "town_ave.columns = ['town', 'town_min_unit_price', 'town_max_unit_price']\n",
    "\n",
    "# Only merged if column not exist - to ease re-running of code\n",
    "if 'town_min_unit_price' not in hex_df.columns:\n",
    "    hex_df = pd.merge(hex_df, town_ave, how='left', on='town')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize price of property with respect to its HDB Township\n",
    "max_ = hex_df['town_max_unit_price']\n",
    "min_ = hex_df['town_min_unit_price']\n",
    "hex_df['appeal_score'] = (hex_df['unit_price'] - min_) / ( max_ - min_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_df.to_csv('./Dataset/final_data_with_hex.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
