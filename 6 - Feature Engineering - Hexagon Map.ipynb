{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singapore Public Housing (HDB) Resale Price Prediction Model (Part 6)\n",
    "### Feature Engineering - Hexagon Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no way to visualize the distribution of sale price geographically better than plotting a heatmap over a Singapore streetmap. The heatmap could show the hotspot in each township, the effect of amenities and infrastructure and probably factors that we have not thought of yet. After a few trials on different technique available in Python, it came to conclusion that plotting the distribution as a choropleth map using Folium is the easiest to customize and without a need to API key/token.\n",
    "\n",
    "This notebook will not covers the plotting part, instead we will now focus on the feature engineering and wrangling of geospatial data using Geojson file. The original raw file was downloaded from [Singapore online archive](https://data.gov.sg/dataset/master-plan-2019-planning-area-boundary-no-sea), where the planning area (similar to township) are plotted into Geojson format. However, there is a minor discrepancy in the planning area (zoned by Urban Development Board, URA) and the actual HDB township. So work was done away from the notebook to add on one more sub-area of Whampoa, which will be merged with Kallang to form the HDB township of Kallang/Whampoa. The original planning area had Whampoa included in Novena instead of Kallang.\n",
    "\n",
    "The general approach is to divide the Singapore map into equal size of hexagon. Then we would label each HDB address into their situated hexagon and obtain a mean value for each hexagon. As such, a heatmap can be plotted with an choropleth overlay with the feature that we want to study in accordance to granularity we prefer. For this project, each hexagon is set to be 250 meters in diameter; however, do feel free to change the diameter in function create_hex_map if you prefer other granularity.\n",
    "\n",
    "Part of this notebook was performed on Google Colab due to known dependencies issues with Geopandas. The code involving geopandas Geojson driver have been commented out for the ease of running the notebook without error. You may uncomment the code if you would like to test the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies for Hexagon-Map Creation\n",
    "# # No need to import\n",
    "# !apt install libspatialindex-dev\n",
    "# !pip install rtree, geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Geospatial Libraries\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "from matplotlib.patches import RegularPolygon\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = pd.read_csv('./Dataset/final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hexagon Map Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the naive-haversine function that was created previously in Part 5, the function below is the actual haversine formula that is more accurate than the previous one. It requires more computing power so it was not used previously. However, in this application, we need all the hexagons to line up edge to edge so accuracy is paramount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate haversine distance between two coordinates\n",
    "def haversine(coord1, coord2):\n",
    "    lon1, lat1 = coord1\n",
    "    lon2, lat2 = coord2\n",
    "    \n",
    "    # Earth radius\n",
    "    R = 6_371_000\n",
    "    phi_1 = np.radians(lat1)\n",
    "    phi_2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a),np.sqrt(1 - a))\n",
    "    meters = R * c\n",
    "    km = meters / 1000.0\n",
    "    meters = round(meters)\n",
    "    km = round(km, 3)\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is to remove planning area that was planned for any HDB development and to combine several planning area into a HDB township. By removing the area that has no HDB at all, we could save up valueable processing power and time computing for hexagon. As mentioned before, Kallang and Whampoa have to be merged to form the HDB township of Kallang/Whampoa. Apart from that, the central area of Singapore divided up into multiple planning area for development efficiency; for our application, we will merged these area into a HDB township of Central Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# # Loading data using Geopandas\n",
    "# df = gpd.read_file('./Raw/planning_area_with_whampoa.json')\n",
    "\n",
    "# # Drop off planning area that are not HDB estate to save up processing power and memory\n",
    "# df = df.drop([0, 1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 16, 18, 19, 25, 26, 29, 30, 34, 39, 44, 51, 53]).reset_index(drop=True)\n",
    "\n",
    "# # Combining smaller planning areas in accordance of HDB estate neighbourhood in raw DataFrame\n",
    "# df.loc[[0, 25, 26, 29, 30], 'name'] = 'CENTRAL AREA'\n",
    "# df.loc[[1, 31], 'name'] = 'KALLANG/WHAMPOA'\n",
    "\n",
    "# # Dissolve area with the same name\n",
    "# TO_boundary = df.dissolve(by='name')\n",
    "\n",
    "# # Export to geojson file for further processing\n",
    "# TO_boundary.to_file('./Dataset/Spatial/planning_area_optimized.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create hexagon polygon on geojson file\n",
    "def create_hex_map(hex_diam=250):\n",
    "    # Import the optimized planning area polygons\n",
    "    df = gpd.read_file('./Dataset/Spatial/planning_area_optimized.geojson')\n",
    "\n",
    "    # Furthest coordinates on Singapore\n",
    "    xmin, ymin, xmax ,ymax = df.total_bounds\n",
    "\n",
    "    # East-West & North-South Length of Singapore\n",
    "    EW = haversine((xmin, ymin), (xmax, ymin))\n",
    "    NS = haversine((xmin, ymin), (xmin, ymax)) + 10000\n",
    "\n",
    "    # Diameter of hexagon in meters\n",
    "    d = hex_diam\n",
    "\n",
    "    # Calculate width of hexagon in meters and grid structure of hexagon\n",
    "    w = d * np.sin(np.pi / 3)\n",
    "    n_cols = int(EW / w) + 1\n",
    "    n_rows = int(NS / d) + 1\n",
    "\n",
    "    # Calculate width of hexagon in coordinates\n",
    "    w = (xmax - xmin) / n_cols\n",
    "\n",
    "    # Calculate diameter of hexagon in coordinates\n",
    "    d = w / np.sin(np.pi / 3)\n",
    "\n",
    "    array_of_hexes = []\n",
    "\n",
    "    # Creating multipolygon for each hexagon\n",
    "    for rows in range(0, n_rows):\n",
    "        hcoord = np.arange(xmin, xmax, w) + (rows % 2) * w / 2\n",
    "        vcoord = [ymax - rows * d * 0.75] * n_cols\n",
    "        for x, y in zip(hcoord, vcoord):\n",
    "            hexes = RegularPolygon((x, y), numVertices=6, radius=d/2, alpha=0.2, edgecolor='k')\n",
    "            verts = hexes.get_path().vertices\n",
    "            trans = hexes.get_patch_transform()\n",
    "            points = trans.transform(verts)\n",
    "            array_of_hexes.append(Polygon(points))\n",
    "\n",
    "    # Transforming multipolygon into geo-dataframe and export to Geojson & CSV\n",
    "    hex_grid = gpd.GeoDataFrame({'geometry': array_of_hexes}, crs={'init': 'epsg:4326'})\n",
    "    TO_hex = gpd.overlay(hex_grid, df)\n",
    "    TO_hex = gpd.GeoDataFrame(TO_hex,geometry='geometry')\n",
    "    TO_hex.to_file('./Dataset/Spatial/planning_area_hexed.geojson', driver='GeoJSON')\n",
    "    TO_hex[['geometry']].to_csv('./Dataset/Spatial/HexMap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# create_hex_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in hex-ed singapore map into Geopandas DataFrame\n",
    "hex_map = pd.read_csv('./Dataset/Spatial/HexMap.csv')\n",
    "hex_map['geometry'] = hex_map['geometry'].apply(wkt.loads)\n",
    "hex_map = gpd.GeoDataFrame(hex_map, geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we will perform some feature engineering to study the true effect of amenities and other features. We will create be creating 3 different type of metrics:\n",
    "\n",
    "1. **price_per_sqft** - The simplest form of unit price in the real estate market\n",
    "2. **unit_price** - It is the price_per_sqft divided by the remaining year lease, eliminating the effect of property lease\n",
    "3. **elem_price** - Price metric that is more elementary than the unit_price where it has factored in the effect of storey range as well\n",
    "\n",
    "We would also remove Terrace HDB houses from the dataset as they proved to be outliers as contrary to the general HDB flats. They are extremely rare in Singapore, in fact only 2 neighbourhoods are captured in the dataset. The inclusion would have negligible effect on the model accuracy as the sample size is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove terrace-house (outliers) from dataset for better mapping visualization\n",
    "hdb = hdb[hdb['flat_model']!='Terrace'].reset_index(drop=True)\n",
    "\n",
    "# Price per square feet\n",
    "hdb['price_per_sqft'] = hdb['resale_price'] / hdb['floor_area_sqm'] * 10.7639\n",
    "\n",
    "# Unit Price --- Price per area per year lease\n",
    "hdb['unit_price'] = hdb['price_per_sqft'] / hdb['remaining_lease']\n",
    "\n",
    "# Elementary Price --- Price per area per year lease per storey level\n",
    "hdb['elem_price'] = hdb['unit_price'] / hdb['storey_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DATA IS COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# # Original method of grouping property entries by address\n",
    "# hex_df = hdb.groupby('address').mean().reset_index()\n",
    "\n",
    "# Renewed method to bypass calculation running\n",
    "hex_df = pd.read_csv('./Dataset/final_data_with_hex.csv')\n",
    "\n",
    "# Label each address to their respective hexagon\n",
    "if hex_df['hex_id'].isnull().sum() > 0:\n",
    "    for idx, (latitude, longitude) in enumerate(zip(hex_df['latitude'], hex_df['longitude'])):\n",
    "        print('\\rWaiting... {} addresses remaining... '.format(len(hex_df)-idx-1), end='.')\n",
    "        if math.isnan(hex_df.loc[idx, 'hex_id']):\n",
    "            for boundary, hex_id in zip(hex_map.geometry, hex_map.hex_id):\n",
    "                coor = Point(longitude, latitude)\n",
    "                if coor.within(boundary):\n",
    "                    hex_df.loc[idx, 'hex_id'] = hex_id\n",
    "                    break\n",
    "            hex_df.to_csv('./Dataset/final_data_with_hex.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print('--- DATA IS COMPLETE ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further optimize the data by taking out all hexagon that has no data entry at all. It would cut down the processing time during mapping significantly, so we could zoom in and out the map with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Optimize by taking only hexagon with address/data\n",
    "hex_map.columns = ['hex_id', 'geometry']\n",
    "hex_map = hex_map[hex_map['hex_id'].isin(hex_df['hex_id'].unique())]\n",
    "\n",
    "# ### ---      KNOWN DEPENDENCIES ISSUE      --- ###\n",
    "# ### ---  CODES TO EXPORT FOR GOOGLE COLAB  --- ###\n",
    "# ### ---     UNCOMMENT TO TEST THE CODE     --- ###\n",
    "\n",
    "# # Export for Colab\n",
    "# hex_map.to_csv('./Dataset/Spatial/HexMap_Optimized.csv', index=False)\n",
    "\n",
    "# # Read from Colab\n",
    "# hex_map = pd.read_csv('./Dataset/Spatial/HexMap_Optimized.csv')\n",
    "# hex_map['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# hex_map = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# # Export the optimized hexmap to geojson format for mapping purpose\n",
    "# hex_map.columns = ['hex_id', 'geometry']\n",
    "# hex_map.to_file('./Dataset/Spatial/hexmap.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any missing points not hex-ed (outliers far from actual HDB estate)\n",
    "# Only 3 addresses affect - 1 x Changi Village + 2 x Farrer Road\n",
    "hex_df = hex_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Only merged if column not exist - to ease re-running of code\n",
    "if 'town' not in hex_df.columns:\n",
    "    hex_df = pd.merge(hex_df, hdb[['address', 'town']], how='left', on='address')\n",
    "\n",
    "# Merging cause dataframe to unpack, drop all duplicates with the same address\n",
    "hex_df = hex_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the general trend where property is usually more expensive when they are closer to the city center, we would not be able to study the heatmap at suburban of Singapore as they will all be at the lower rung for all price metrics. Increasing the granularities would cause more harm than good on area-centric study. So one of the method that we have adopted is to calculate a township-wide price index.\n",
    "\n",
    "It is dubbed as 'Appeal Score' in this project, where it is a measure of how property appeal to home buyer at its township. Mathematically, it is unit price (resale price but stripped away the effect of lease and floor area) that is normalized to a scale of 0 to 1 based on the local minimum and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate HDB-town-wide local Minimum and Maximum\n",
    "town_ave = hex_df.groupby('town').agg(['min', 'max'])['unit_price'].reset_index()\n",
    "town_ave.columns = ['town', 'town_min_unit_price', 'town_max_unit_price']\n",
    "\n",
    "# Only merged if column not exist - to ease re-running of code\n",
    "if 'town_min_unit_price' not in hex_df.columns:\n",
    "    hex_df = pd.merge(hex_df, town_ave, how='left', on='town')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize price of property with respect to its HDB Township\n",
    "max_ = hex_df['town_max_unit_price']\n",
    "min_ = hex_df['town_min_unit_price']\n",
    "hex_df['appeal_score'] = (hex_df['unit_price'] - min_) / ( max_ - min_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_df.to_csv('./Dataset/final_data_with_hex.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready for the analysis and modelling. It will be continued in the last notebook, Part 7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
